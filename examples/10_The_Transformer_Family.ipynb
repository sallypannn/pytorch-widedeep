{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment there are 3 transformer-based algorithms available. \n",
    "\n",
    "Here are examples of how to use them\n",
    "\n",
    "Perhaps the main comment is that when using transformer-based models, the data preparation is a bit different than in other models. Therefore one needs to know the set up at pre-processing stage. \n",
    "\n",
    "Let's have a look, starting with the `TabTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javier/.pyenv/versions/3.7.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.training import Trainer\n",
    "from pytorch_widedeep.models import TabTransformer, SAINT, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/adult/adult.csv.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education      marital_status  \\\n",
       "0   25    Private  226802          11th       Never-married   \n",
       "1   38    Private   89814       HS-grad  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital_gain  capital_loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours_per_week native_country  target  \n",
       "0              40  United-States       0  \n",
       "1              50  United-States       0  \n",
       "2              40  United-States       1  \n",
       "3              40  United-States       1  \n",
       "4              30  United-States       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience, we'll replace '-' with '_'\n",
    "df.columns = [c.replace(\"-\", \"_\") for c in df.columns]\n",
    "#binary target\n",
    "df['target'] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "df.drop([\"income\", \"educational_num\"], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols, cont_cols = [], []\n",
    "for col in df.columns:\n",
    "    # 50 is just a random number I choose here for this example\n",
    "    if df[col].dtype == \"O\" or df[col].nunique() < 50 and col != \"target\":\n",
    "        cat_cols.append(col)\n",
    "    elif col != \"target\":        \n",
    "        cont_cols.append(col)\n",
    "target_col = \"target\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Standard\" `TabTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[target_col].values\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_cols, \n",
    "                                   continuous_cols=cont_cols, \n",
    "                                   for_transformer=True\n",
    "                                  )\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here all categorical columns will be encoded as 32 dim embeddings, then passed through the transformer \n",
    "# blocks, concatenated with the continuous and finally through an MLP\n",
    "tab_transformer = TabTransformer(column_idx=tab_preprocessor.column_idx,\n",
    "                                 embed_input=tab_preprocessor.embeddings_input,\n",
    "                                 continuous_cols=tab_preprocessor.continuous_cols, \n",
    "                                 cont_norm_layer=\"layernorm\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabTransformer(\n",
       "  (cat_embed): Embedding(103, 32, padding_idx=0)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cont_norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "  (transformer_blks): Sequential(\n",
       "    (block0): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block1): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block2): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block3): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block4): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block5): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=261, out_features=1044, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=1044, out_features=522, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(deeptabular=tab_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 153/153 [00:23<00:00,  6.54it/s, loss=0.375, metrics={'acc': 0.8247}]\n",
      "valid: 100%|██████████| 39/39 [00:01<00:00, 19.74it/s, loss=0.356, metrics={'acc': 0.8375}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(X_tab=X_tab, target=target, n_epochs=1, batch_size=256, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to use the `FT-Transformer` (just set `embed_continuous=True`), where continuous cols are also represented by \"Embeddings\", via a 1 layer MLP (with or without activation function). When using the `FT-Transformer` we can choose to use the `[CLS]` token as a pooling method or concatenate the output from the transformer blocks, as we did before. Let's use here the `[CLS]` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_cols, \n",
    "                                   continuous_cols=cont_cols, \n",
    "                                   for_transformer=True,\n",
    "                                   with_cls_token=True\n",
    "                                  )\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here all categorical columns will be encoded as 32 dim embeddings, then passed through the transformer \n",
    "# blocks, concatenated with the continuous and finally through an MLP\n",
    "ft_transformer = TabTransformer(column_idx=tab_preprocessor.column_idx,\n",
    "                                 embed_input=tab_preprocessor.embeddings_input,\n",
    "                                 continuous_cols=tab_preprocessor.continuous_cols, \n",
    "                                 embed_continuous=True,\n",
    "                                 embed_continuous_activation=None,                            \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabTransformer(\n",
       "  (cat_embed): Embedding(104, 32, padding_idx=0)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cont_norm): Identity()\n",
       "  (cont_embed): ContinuousEmbeddings()\n",
       "  (transformer_blks): Sequential(\n",
       "    (block0): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block1): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block2): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block3): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block4): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block5): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeep(deeptabular=ft_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 153/153 [00:38<00:00,  4.02it/s, loss=0.412, metrics={'acc': 0.8002}]\n",
      "valid: 100%|██████████| 39/39 [00:02<00:00, 14.69it/s, loss=0.326, metrics={'acc': 0.8524}]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(X_tab=X_tab, target=target, n_epochs=1, batch_size=256, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can choose to use SAINT, with its inter-sample attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saint = SAINT(column_idx=tab_preprocessor.column_idx,\n",
    "              embed_input=tab_preprocessor.embeddings_input,\n",
    "              continuous_cols=tab_preprocessor.continuous_cols, \n",
    "              transformer_activation=\"geglu\",\n",
    "              embed_continuous=True,\n",
    "              embed_continuous_activation=None,                            \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAINT(\n",
       "  (cat_embed): Embedding(104, 32, padding_idx=0)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cont_norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "  (cont_embed): ContinuousEmbeddings()\n",
       "  (transformer_blks): Sequential(\n",
       "    (block0): SaintEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (self_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (self_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (self_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=448, out_features=1344, bias=True)\n",
       "        (out_proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "      )\n",
       "      (row_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=448, out_features=3584, bias=True)\n",
       "        (w_2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (row_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block1): SaintEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (self_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (self_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (self_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=448, out_features=1344, bias=True)\n",
       "        (out_proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "      )\n",
       "      (row_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=448, out_features=3584, bias=True)\n",
       "        (w_2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (row_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block2): SaintEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (self_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (self_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (self_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=448, out_features=1344, bias=True)\n",
       "        (out_proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "      )\n",
       "      (row_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=448, out_features=3584, bias=True)\n",
       "        (w_2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (row_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block3): SaintEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (self_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (self_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (self_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=448, out_features=1344, bias=True)\n",
       "        (out_proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "      )\n",
       "      (row_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=448, out_features=3584, bias=True)\n",
       "        (w_2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (row_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block4): SaintEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (self_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (self_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (self_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=448, out_features=1344, bias=True)\n",
       "        (out_proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "      )\n",
       "      (row_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=448, out_features=3584, bias=True)\n",
       "        (w_2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (row_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block5): SaintEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (self_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=256, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (self_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (self_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=448, out_features=1344, bias=True)\n",
       "        (out_proj): Linear(in_features=448, out_features=448, bias=True)\n",
       "      )\n",
       "      (row_attn_ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=448, out_features=3584, bias=True)\n",
       "        (w_2): Linear(in_features=1792, out_features=448, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GEGLU()\n",
       "      )\n",
       "      (row_attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (row_attn_ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 306/306 [02:34<00:00,  1.98it/s, loss=0.555, metrics={'acc': 0.7607}]\n",
      "valid: 100%|██████████| 77/77 [00:08<00:00,  8.96it/s, loss=0.551, metrics={'acc': 0.7607}]\n"
     ]
    }
   ],
   "source": [
    "model = WideDeep(deeptabular=saint)\n",
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy])\n",
    "trainer.fit(X_tab=X_tab, target=target, n_epochs=1, batch_size=128, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final comment is that all 3 transformer-based model have the option of using the so called \"Shared Embeddings\". The idea behind the shared embeddings is explained in the original TabTransformer paper and also here in this [post](https://jrzaurin.github.io/infinitoml/2021/02/18/pytorch-widedeep_iii.html).\n",
    "\n",
    "For transformer-based models this implies a bit of a different data preparation process since each column will be encoded individually (programmatically is way easier to implement) and the use of shared embeddings needs to be specified at preprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_cols, \n",
    "                                   continuous_cols=cont_cols, \n",
    "                                   for_transformer=True,\n",
    "                                   shared_embed=True,\n",
    "                                   with_cls_token=True\n",
    "                                  )\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_transformer = TabTransformer(column_idx=tab_preprocessor.column_idx,\n",
    "                                embed_input=tab_preprocessor.embeddings_input,\n",
    "                                continuous_cols=tab_preprocessor.continuous_cols, \n",
    "                                embed_continuous=True,\n",
    "                                embed_continuous_activation=None,       \n",
    "                                shared_embed=True,  \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabTransformer(\n",
       "  (cat_embed): ModuleDict(\n",
       "    (emb_layer_cls_token): SharedEmbeddings(\n",
       "      (embed): Embedding(1, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_education): SharedEmbeddings(\n",
       "      (embed): Embedding(17, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_gender): SharedEmbeddings(\n",
       "      (embed): Embedding(3, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_marital_status): SharedEmbeddings(\n",
       "      (embed): Embedding(8, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_native_country): SharedEmbeddings(\n",
       "      (embed): Embedding(43, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_occupation): SharedEmbeddings(\n",
       "      (embed): Embedding(16, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_race): SharedEmbeddings(\n",
       "      (embed): Embedding(6, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_relationship): SharedEmbeddings(\n",
       "      (embed): Embedding(7, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_layer_workclass): SharedEmbeddings(\n",
       "      (embed): Embedding(10, 32, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (cont_norm): Identity()\n",
       "  (cont_embed): ContinuousEmbeddings()\n",
       "  (transformer_blks): Sequential(\n",
       "    (block0): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block1): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block2): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block3): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block4): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (block5): TransformerEncoder(\n",
       "      (self_attn): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (inp_proj): Linear(in_features=32, out_features=96, bias=True)\n",
       "        (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (ff): PositionwiseFF(\n",
       "        (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU()\n",
       "      )\n",
       "      (attn_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff_addnorm): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer_mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 153/153 [00:35<00:00,  4.32it/s, loss=0.411, metrics={'acc': 0.8036}]\n",
      "valid: 100%|██████████| 39/39 [00:02<00:00, 15.03it/s, loss=0.319, metrics={'acc': 0.8583}]\n"
     ]
    }
   ],
   "source": [
    "model = WideDeep(deeptabular=ft_transformer)\n",
    "trainer = Trainer(model, objective='binary', metrics=[Accuracy])\n",
    "trainer.fit(X_tab=X_tab, target=target, n_epochs=1, batch_size=256, val_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
